#!/usr/bin/env python3
"""
Interface SUI avec communication orale en temps r√©el utilisant WhisperX.

Cette interface permet :
- Communication orale bidirectionnelle en fran√ßais
- Transcription en temps r√©el avec gestion des pauses
- Affichage progressif de la transcription
- Batching intelligent des segments de parole
- Gestion des interruptions et reprises
"""

import os
import sys
import time
import threading
import logging
from typing import Optional, Dict, Any
from pathlib import Path

# Configuration pour √©viter les warnings OMP
os.environ["OMP_NUM_THREADS"] = "1"

# Configuration des logs
logging.basicConfig(
    level=logging.DEBUG,  # Chang√© de INFO √† DEBUG pour plus de d√©tails
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(os.path.expanduser('~/.peer/sui_realtime.log'), mode='a')
    ]
)

# Ajouter le chemin source pour l'importation
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(current_dir, 'src'))
sys.path.insert(0, current_dir)

from peer.interfaces.sui.stt.speech_recognizer import SpeechRecognizer
from peer.interfaces.sui.stt.audio_io import AudioCapture, VoiceActivityDetector, VADMode, AudioFormat
from peer.interfaces.sui.stt.continuous_speech_manager import ContinuousSpeechManager
from peer.interfaces.sui.tts.simple_tts_engine import SimpleTTS
from peer.interfaces.sui.domain.models import SpeechRecognitionResult


class RealTimeSpeechInterface:
    """Interface de communication orale en temps r√©el avec WhisperX."""
    
    def __init__(self):
        self.logger = logging.getLogger("RealTimeSpeechInterface")
        
        # √âtat de l'interface
        self.running = False
        self.listening = False
        self.speaking = False
        self._last_speech_time = 0  # Pour suivre quand l'IA a fini de parler
        self._last_spoken_text = ""  # Pour d√©tecter les √©chos
        
        # Composants
        self.speech_recognizer: Optional[SpeechRecognizer] = None
        self.audio_capture: Optional[AudioCapture] = None
        self.vad: Optional[VoiceActivityDetector] = None
        self.speech_manager: Optional[ContinuousSpeechManager] = None
        self.tts_engine: Optional[SimpleTTS] = None
        
        # Affichage
        self.current_transcription = ""
        self.last_final_transcription = ""
        self.transcription_display_lock = threading.Lock()
        
        # M√©triques
        self.session_stats = {
            'start_time': time.time(),
            'total_speech_time': 0.0,
            'transcriptions_count': 0,
            'words_transcribed': 0
        }
        
        self._init_components()
    
    def _init_components(self):
        """Initialise tous les composants n√©cessaires."""
        self.logger.info("üöÄ Initialisation de l'interface de communication orale en temps r√©el")
        
        try:
            # Configuration WhisperX optimis√©e pour le temps r√©el
            stt_config = {
                'stt_settings': {
                    'engines': {
                        'whisperx': {
                            'enabled': True,
                            'model_name': 'base',  # Mod√®le rapide pour temps r√©el
                            'language': 'fr',
                            'priority': 1,
                            'parameters': {
                                'batch_size': 8,    # Batch plus petit pour vitesse
                                'task': 'transcribe',
                                'language': 'french'
                            }
                        }
                    }
                }
            }
            
            # Initialiser le recognizer
            self.speech_recognizer = SpeechRecognizer(stt_config)
            self.logger.info("‚úÖ Speech recognizer initialis√©")
            
            # Initialiser la capture audio
            audio_config = {
                'sample_rate': AudioFormat.SAMPLE_RATE,
                'channels': AudioFormat.CHANNELS,
                'chunk_size': AudioFormat.CHUNK_SIZE,
                'vad_sensitivity': 3  # Mode VERY_AGGRESSIVE pour maximiser la d√©tection
            }
            self.audio_capture = AudioCapture(audio_config)
            self.logger.info("‚úÖ Capture audio initialis√©e")
            
            # Initialiser le VAD (Voice Activity Detection)
            self.vad = VoiceActivityDetector(mode=VADMode.VERY_AGGRESSIVE)
            self.logger.info("‚úÖ VAD initialis√©")
            
            # Initialiser le gestionnaire de parole continue
            self.speech_manager = ContinuousSpeechManager(
                speech_recognizer=self.speech_recognizer,
                pause_threshold=0.9,      # Seuil de pause r√©duit pour √™tre plus r√©actif aux pauses
                min_segment_duration=0.05, # Segments courts pour capturer toute parole
                max_batch_duration=5.0,   # Batches plus courts pour r√©duire la latence
                transcription_callback=self._on_transcription_received
            )
            self.logger.info("‚úÖ Gestionnaire de parole continue initialis√©")
            
            # Initialiser le TTS pour les r√©ponses
            from peer.interfaces.sui.tts.base import TTSConfig, TTSEngineType
            tts_config = TTSConfig(
                engine_type=TTSEngineType.SIMPLE,
                language='fr',
                voice='Audrey (Premium)'
            )
            self.tts_engine = SimpleTTS(tts_config)
            self.logger.info("‚úÖ Moteur TTS initialis√©")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
    
    def start(self):
        """D√©marre l'interface de communication orale."""
        if self.running:
            self.logger.warning("Interface d√©j√† en cours")
            return
        
        self.running = True
        self.logger.info("üéôÔ∏è D√©marrage de l'interface de communication orale")
        
        # D√©marrer les composants
        self.speech_manager.start()
        
        # Message de bienvenue
        welcome_message = (
            "Interface de communication orale avec WhisperX d√©marr√©e. "
            "Vous pouvez maintenant parler et voir la transcription en temps r√©el. "
            "Dites 'arr√™ter' pour terminer."
        )
        self._speak_async(welcome_message)
        
        # Afficher l'interface
        self._display_interface()
        
        # D√©marrer l'√©coute audio
        self._start_audio_listening()
        
        # Boucle principale avec affichage temps r√©el
        self._main_loop()
    
    def stop(self):
        """Arr√™te l'interface de communication orale."""
        self.running = False
        self.listening = False
        
        if self.speech_manager:
            self.speech_manager.stop()
        
        if self.audio_capture:
            self.audio_capture.stop_recording()
        
        self.logger.info("üõë Interface de communication orale arr√™t√©e")
        
        # Afficher les statistiques finales
        self._display_final_stats()
    
    def _start_audio_listening(self):
        """D√©marre l'√©coute audio avec traitement en temps r√©el."""
        self.listening = True
        
        # D√©marrer l'enregistrement
        if not self.audio_capture.start_recording():
            self.logger.error("‚ùå Impossible de d√©marrer l'enregistrement audio")
            return
        
        # Thread pour traiter les segments audio en continu
        def audio_processing_loop():
            """Boucle de traitement des segments audio."""
            segment_count = 0
            speech_count = 0
            
            while self.running and self.listening:
                if self.speaking:
                    # Pause compl√®te pendant que l'IA parle
                    time.sleep(0.1)
                    
                    # Vider la file d'attente audio pour √©viter les segments captur√©s pendant la parole
                    try:
                        while not self.audio_capture.audio_queue.empty():
                            self.audio_capture.audio_queue.get_nowait()
                    except:
                        pass
                        
                    continue
                
                # R√©cup√©rer le prochain segment audio
                segment = self.audio_capture.get_audio_segment(timeout=0.5)
                if segment is None:
                    continue
                
                # D√©tecter les segments trop faibles qui pourraient √™tre des √©chos
                # Ignorer les segments de faible √©nergie juste apr√®s avoir parl√©
                recently_spoke = hasattr(self, '_last_speech_time') and \
                               (time.time() - self._last_speech_time < 1.0)
                
                if recently_spoke and segment.energy_level < 0.01:
                    # Ignorer les segments probablement caus√©s par un √©cho
                    self.logger.debug(f"üîá √âcho potentiel ignor√©: √©nergie={segment.energy_level:.4f}")
                    continue
                
                segment_count += 1
                if segment.has_speech:
                    # Appliquer un seuil d'√©nergie plus √©lev√© juste apr√®s avoir parl√©
                    energy_threshold = 0.005
                    if recently_spoke:
                        energy_threshold = 0.015  # Seuil plus √©lev√© apr√®s parole
                    
                    if segment.energy_level >= energy_threshold:
                        speech_count += 1
                        self.logger.debug(f"üé§ Segment valide: √©nergie={segment.energy_level:.4f}")
                    else:
                        # Forcer has_speech √† False pour les segments de faible √©nergie
                        segment.has_speech = False
                        self.logger.debug(f"üîá Segment ignor√© (√©nergie trop faible): {segment.energy_level:.4f}")
                        continue
                
                # Log p√©riodique pour diagnostiquer
                if segment_count % 50 == 0:
                    self.logger.info(f"üìä Segments trait√©s: {segment_count}, avec parole: {speech_count}")
                    # Forcer la finalisation uniquement si n√©cessaire
                    if self.speech_manager and hasattr(self.speech_manager, 'current_batch'):
                        current_batch = self.speech_manager.current_batch
                        if current_batch and time.time() - current_batch.last_activity_time > 1.0:
                            self.logger.info("üîÑ Finalisation forc√©e apr√®s pause d√©tect√©e")
                            self.speech_manager.force_finalize_current_batch()
                
                # D√©tecter les pauses naturelles pour finaliser rapidement
                last_segment_time = getattr(segment, 'end_time', time.time())
                pause_since_last = time.time() - last_segment_time
                if pause_since_last > 1.0 and not segment.has_speech and self.speech_manager and hasattr(self.speech_manager, 'force_finalize_current_batch'):
                    # Finaliser si une pause de plus de 1s est d√©tect√©e
                    self.speech_manager.force_finalize_current_batch()
                    self.logger.info(f"üîÑ Finalisation forc√©e apr√®s pause de {pause_since_last:.1f}s")
                
                # Convertir en format appropri√© pour le gestionnaire de parole
                # Le segment contient d√©j√† les donn√©es en float32
                audio_np = segment.data
                has_speech = segment.has_speech
                
                self.logger.debug(f"üé§ Segment audio: {len(audio_np)} √©chantillons, parole={has_speech}, √©nergie={segment.energy_level:.3f}")
                
                # Envoyer au gestionnaire de parole continue
                try:
                    self.speech_manager.add_audio_segment(audio_np, has_speech)
                except Exception as e:
                    self.logger.error(f"‚ùå Erreur traitement audio: {e}")
        
        # D√©marrer le thread de traitement audio
        audio_thread = threading.Thread(target=audio_processing_loop, daemon=True)
        audio_thread.start()
        
        self.logger.info("üëÇ √âcoute audio d√©marr√©e")
    
    def _on_transcription_received(self, text: str, is_final: bool):
        """Callback appel√© quand une transcription est re√ßue."""
        # Ignorer les transcriptions vides ou trop courtes
        if not text or len(text.strip()) < 2:
            self.logger.debug(f"‚è≠Ô∏è Transcription ignor√©e (trop courte): '{text}'")
            return
            
        # Ignorer les transcriptions qui ressemblent √† des √©chos de ce que l'IA vient de dire
        if hasattr(self, '_last_spoken_text') and self._last_spoken_text:
            # Calculer la similarit√© entre la transcription et la derni√®re chose dite par l'IA
            similarity = self._text_similarity(text.lower(), self._last_spoken_text.lower())
            
            # Si la transcription ressemble beaucoup √† ce que l'IA vient de dire, l'ignorer
            if similarity > 0.5:  # Plus de 50% de similarit√© = probablement un √©cho
                self.logger.info(f"üîá √âcho probable ignor√© (similarit√© {similarity:.2f}): '{text}'")
                return
        
        with self.transcription_display_lock:
            if is_final:
                # Transcription finale
                self.last_final_transcription = text
                self.current_transcription = ""
                
                # Mettre √† jour les stats
                self.session_stats['transcriptions_count'] += 1
                self.session_stats['words_transcribed'] += len(text.split())
                
                self.logger.info(f"‚úÖ Transcription finale: '{text}'")
                
                # Traiter la commande
                self._process_voice_command(text)
                
            else:
                # Transcription partielle
                self.current_transcription = text
        
        # Mettre √† jour l'affichage
        self._update_display()
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """
        Calcule la similarit√© entre deux textes.
        Retourne une valeur entre 0 (compl√®tement diff√©rents) et 1 (identiques).
        """
        # M√©thode simple: v√©rifier combien de mots sont communs
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
            
        common_words = words1.intersection(words2)
        total_unique_words = len(words1.union(words2))
        
        return len(common_words) / total_unique_words if total_unique_words > 0 else 0.0
    
    def _process_voice_command(self, text: str):
        """Traite une commande vocale re√ßue."""
        text_lower = text.lower().strip()
        
        self.logger.info(f"üó£Ô∏è Commande re√ßue: '{text}'")
        
        # Commandes sp√©ciales
        if any(word in text_lower for word in ['arr√™ter', 'arr√™t', 'stop', 'quitter']):
            response = "D'accord, j'arr√™te l'interface. Au revoir!"
            self._last_spoken_text = response  # M√©moriser pour √©viter l'√©cho
            self._speak_async(response)
            # Attendre que la synth√®se termine avant d'arr√™ter
            time.sleep(2)
            self.stop()
            return
        
        if any(word in text_lower for word in ['aide', 'help', 'aidez-moi']):
            response = (
                "Je vous √©coute en continu et transcris ce que vous dites. "
                "Vous pouvez parler naturellement, je g√®re les pauses. "
                "Dites 'arr√™ter' pour terminer l'interface."
            )
            self._last_spoken_text = response  # M√©moriser pour √©viter l'√©cho
            self._speak_async(response)
            return
        
        if any(word in text_lower for word in ['heure', 'temps', 'quelle heure']):
            import datetime
            now = datetime.datetime.now()
            response = f"Il est {now.strftime('%H heures %M')}."
            self._last_spoken_text = response  # M√©moriser pour √©viter l'√©cho
            self._speak_async(response)
            return
        
        if any(word in text_lower for word in ['statistiques', 'stats', 'm√©triques']):
            stats = self.speech_manager.get_stats()
            response = (
                f"Statistiques de la session: "
                f"{stats['segments_processed']} segments trait√©s, "
                f"{stats['batches_completed']} batches transcrits, "
                f"{self.session_stats['transcriptions_count']} transcriptions finales."
            )
            self._last_spoken_text = response  # M√©moriser pour √©viter l'√©cho
            self._speak_async(response)
            return
        
        # R√©ponse g√©n√©rale
        response = f"J'ai entendu: {text}. Comment puis-je vous aider?"
        self._last_spoken_text = response  # M√©moriser pour √©viter l'√©cho
        self._speak_async(response)
    
    def _speak_async(self, text: str):
        """Synth√®se vocale asynchrone."""
        def speak_worker():
            # Marquer comme parlant pour emp√™cher l'√©coute
            self.speaking = True
            self.logger.info(f"üîä TTS: '{text}'")
            
            try:
                # Vider la file d'attente audio avant de parler pour √©viter les √©chos
                if hasattr(self.audio_capture, 'audio_queue'):
                    try:
                        while not self.audio_capture.audio_queue.empty():
                            self.audio_capture.audio_queue.get_nowait()
                    except:
                        pass
                
                # Effectuer la synth√®se vocale
                start_time = time.time()
                result = self.tts_engine.synthesize(text)
                
                if result.success:
                    # Calculer la dur√©e approximative de la parole (~ 15 caract√®res/seconde)
                    speech_duration = len(text) / 15.0
                    actual_duration = time.time() - start_time
                    
                    # Attendre un peu apr√®s la fin de la parole pour √©viter les √©chos
                    # Au moins 500ms ou 20% de la dur√©e de parole, selon le plus grand
                    cooldown = max(0.5, speech_duration * 0.2)
                    self.logger.debug(f"‚è±Ô∏è Attente post-parole: {cooldown:.2f}s")
                    time.sleep(cooldown)
                    
                    # Marquer le temps de fin de parole pour la d√©tection d'√©cho
                    self._last_speech_time = time.time()
                else:
                    self.logger.error(f"‚ùå Erreur TTS: {result.message}")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur TTS: {e}")
            finally:
                # R√©initialiser l'√©tat de parole
                self.speaking = False
                self.logger.debug("üéôÔ∏è Reprise de l'√©coute")
        
        threading.Thread(target=speak_worker, daemon=True).start()
    
    def _display_interface(self):
        """Affiche l'interface utilisateur."""
        print("\n" + "="*80)
        print("üéôÔ∏è INTERFACE DE COMMUNICATION ORALE TEMPS R√âEL - WHISPERX")
        print("="*80)
        print("üìã Commandes disponibles:")
        print("   ‚Ä¢ Parlez naturellement, je vous √©coute en continu")
        print("   ‚Ä¢ 'aide' - Afficher l'aide")
        print("   ‚Ä¢ 'heure' - Demander l'heure")
        print("   ‚Ä¢ 'statistiques' - Voir les m√©triques")
        print("   ‚Ä¢ 'arr√™ter' - Terminer l'interface")
        print("\nüî¥ √âTAT: En √©coute... Vous pouvez commencer √† parler.\n")
    
    def _update_display(self):
        """Met √† jour l'affichage de la transcription."""
        with self.transcription_display_lock:
            # Effacer les lignes pr√©c√©dentes
            print("\r" + " " * 100, end="\r")
            
            # Afficher la transcription
            if self.current_transcription:
                print(f"üìù [En cours]: {self.current_transcription}", end="\r")
            elif self.last_final_transcription:
                print(f"‚úÖ [Final]: {self.last_final_transcription}")
                print("üî¥ En √©coute...", end="\r")
    
    def _main_loop(self):
        """Boucle principale de l'interface."""
        try:
            while self.running:
                time.sleep(0.1)
                
                # Mise √† jour p√©riodique de l'affichage
                if time.time() % 5 < 0.1:  # Toutes les 5 secondes
                    self._show_status()
                
        except KeyboardInterrupt:
            print("\n‚å®Ô∏è Interruption clavier d√©tect√©e")
            self.stop()
    
    def _show_status(self):
        """Affiche le statut p√©riodique."""
        if self.speech_manager:
            stats = self.speech_manager.get_stats()
            session_duration = time.time() - self.session_stats['start_time']
            
            status = (
                f"‚è±Ô∏è Session: {session_duration:.0f}s | "
                f"üìä Segments: {stats['segments_processed']} | "
                f"üìù Transcriptions: {self.session_stats['transcriptions_count']} | "
                f"üìà Mots: {self.session_stats['words_transcribed']}"
            )
            
            # Afficher en bas de l'√©cran sans perturber la transcription
            print(f"\n{status}", end="\r")
    
    def _display_final_stats(self):
        """Affiche les statistiques finales."""
        session_duration = time.time() - self.session_stats['start_time']
        stats = self.speech_manager.get_stats() if self.speech_manager else {}
        
        print("\n\n" + "="*80)
        print("üìä STATISTIQUES DE LA SESSION")
        print("="*80)
        print(f"‚è±Ô∏è Dur√©e totale: {session_duration:.1f} secondes")
        print(f"üìù Transcriptions finales: {self.session_stats['transcriptions_count']}")
        print(f"üìà Mots transcrits: {self.session_stats['words_transcribed']}")
        print(f"üîÑ Segments trait√©s: {stats.get('segments_processed', 0)}")
        print(f"üì¶ Batches compl√©t√©s: {stats.get('batches_completed', 0)}")
        print(f"‚ö° Temps moy. transcription: {stats.get('avg_transcription_time', 0):.2f}s")
        
        if self.session_stats['transcriptions_count'] > 0:
            wpm = (self.session_stats['words_transcribed'] * 60) / session_duration
            print(f"üéØ Vitesse moyenne: {wpm:.1f} mots/minute")
        
        print("\n‚úÖ Session termin√©e. Merci d'avoir utilis√© l'interface!")


def main():
    """Point d'entr√©e principal."""
    try:
        print("üöÄ D√©marrage de l'interface de communication orale temps r√©el...")
        
        # Cr√©er et d√©marrer l'interface
        interface = RealTimeSpeechInterface()
        interface.start()
        
    except KeyboardInterrupt:
        print("\n‚å®Ô∏è Arr√™t demand√© par l'utilisateur")
    except Exception as e:
        print(f"‚ùå Erreur fatale: {e}")
        logging.error(f"Erreur fatale dans l'interface: {e}")
    
    print("\nüëã Au revoir!")


if __name__ == "__main__":
    main()
